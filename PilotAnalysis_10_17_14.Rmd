---
title: "Viral Pneumonia Pilot Analysis"
author: "Dominic LaRoche"
date: "Friday, October 17, 2014"
output: word_document
---



```{r readinClean,echo=FALSE,message=FALSE}
rm(list=ls())
library(xlsx)
library(BDSS)
library(car)
dat<-read.xlsx("C:/Projects/Pneumonia/DATA FOR ANALYSIS.xlsx",sheetName="DATA",startRow=2,header=F)
names(dat)<-c("s.no","roll.num","Mortality","rvp","other.infect","age","age60","sex","BMI30","BMI","prev.stay","immun.comp","lung.dis","diab","curb","curb35","psiAdmit","psiAdmit4","vpdose","aki","pl.eff","dysp","wheeze","admit","hct30","hct","pna","mech.vent","ecmo")
#fix coding error in mortality
dat$Mortality<-Recode(dat$Mortality,"'NO '='NO'")
#curb score has only 5 levels (0-4) so may best be used as a category
dat$curb35<-Recode(dat$curb35,"'N'='NO'")
dat$curbf<-as.factor(dat$curb)

#need to create binary other infect variable and immumo-comprimised variable
dat$other.infect.stat<-ifelse(dat$other.infect=="NO","NO","YES")
dat$immun.comp.stat<-ifelse(dat$immun.comp=="NO","NO","YES")
#need to separate out PSI and PSI class
sep<-do.call(rbind,strsplit(as.character(dat$psiAdmit),split="\\("))
dat$psi<-as.numeric(as.character(sep[,1]))
dat$psiClass<-substr(sep[,2],1,1)
#need to separate lung disease status from cause
dat$lung.dis.stat<-grepl("YES",dat$lung.dis)
#need to remove character from age and convert to numeric
dat$age<-as.numeric(as.character(Recode(dat$age,"'>90'=90")))
#need to fix AKI 0.4
dat$aki<-factor(Recode(dat$aki,"'0.4'='NO'"))
#need to fix pna= NORMAL
dat$pna<-factor(Recode(dat$pna,"'NORMAL'='NO'"))
dat$rvp<-factor(Recode(dat$rvp,"'RHINOVIRUS '='RHINOVIRUS';c('INFUENZA A 2009 H1N1','INFLUENZA A ')='INFLUENZA A 2009 H1N1'"))
```

#Introduction

This report summarizes a pilot analysis on the data provided on 11 October 2014.  The data is summarized in table 1.  I have initially taken two approaches to this problem: 1) regular LASSO, and 2) grouped LASSO.  I describe the two methods in the corresponding sections below.


```{r SummTable,warning=FALSE,echo=FALSE,eval=FALSE}
SummaryTable(data=dat,colvar="Mortality",rowvars=c("rvp","other.infect.stat","age60","sex","BMI30","prev.stay","immun.comp.stat","lung.dis.stat","diab","curbf","curb35","psiClass","vpdose","aki","pl.eff","dysp","wheeze","admit","hct30","pna","mech.vent","ecmo"), cont.vars=c("age","BMI","psi","hct"), row.names=c("Resp.Panel","Other Infect.","Age>60","Sex","BMI>30","Previous Loc.","Immuno-comp","Pulm. Dis.","Diabetes","Curb Score","Curb>=3/5","PSI Class","Vaspprssor","AKI","PL.Effusion","Dyspnea","Wheezing","Admitted from","HCT<30","PNA","Mech. Vent.","ECMO"),output="rtf",outfile="c:\\Projects\\Pneumonia\\table1.rtf")
```



#Standard LASSO Regression

I first fit a model using standard LASSO regression with the "L1Norm" penalty.  This method creates a separate indicator variable for every level of every factor.  In this data set that creates 40 unique parameters to estimate (including the intercept).  The LASSO penalty shrinks the coefficients individually so you may end up with a non-zero coefficient for a single factor level while the other levels have "no effect".  The practical implications of this method are that you can then create a single variable in the final model using just the indicator for that particular factor level. E.g., if you had a factor that coded for shirt color with levels 'red', 'blue', and 'green', you may only get en effect for green shirts.  In the final model you would only indicate whether the patient was wearing a green shirt or not.  The downside of this method is that if the lone factor level does not make sense in the absence of the other levels then this doesn't work.  The other thing to keep in mind is that the reference category (the category to which the effect of the level is compared) is very important in this method so careful attention must be paid to making a sensible reference category to which the other categories are all compared.  I should also note that this only applies to factors with more than 2 levels, e.g. sex would not be affected.

```{r FitInitModel,echo=FALSE,message=FALSE}
#first using lars to identify the individual dummy coeffs that are important- then group lasso to identify the factors
library(lars)
library(ROCR)
X<-with(dat,model.matrix(~rvp+other.infect.stat+sex+prev.stay+immun.comp.stat+lung.dis.stat+diab+curbf+vpdose+aki+pl.eff+dysp+wheeze+admit+pna+mech.vent+ecmo+age+BMI30+psi+hct))
Y<-ifelse(dat$Mortality=="YES",1,0)
m1<-lars(x=X,y=as.matrix(Y),type="lasso", use.Gram=FALSE)
plot(m1,breaks=F)
```

Figure 1. Plot of the regularization paths for the coefficients in the model.  This shows how the number of non-zero coefficients decreased with an increasing penalty (right to left)


```{r FitCV,echo=FALSE}
set.seed(37)
m1cv<-cv.lars(x=X,y=as.matrix(Y),K=30)#,index=seq(0,.4,.001))
meanCoef<-apply(coef(m1),2,mean)
rankedVars<-sort(abs(meanCoef))
lamFrac<-m1$lambda/max(m1$lambda)
LowestMSE<-coef(m1)[which((m1cv$cv+m1cv$cv.error)==min(m1cv$cv+m1cv$cv.error)),]
m1mat<-cbind(meanCoef,LowestMSE)
m1preds<-predict(m1,X,type="fit",mode="fraction",s=lamFrac[which((m1cv$cv+m1cv$cv.error)==min(m1cv$cv+m1cv$cv.error))])#.18
dat$m1preds<-m1preds$fit
dat$Y<-Y
dat$m1predsind<-ifelse(dat$m1preds<.5,0,1)
pred<-prediction(predictions=dat$m1preds,labels=dat$Y)
#perfect prediction!
perf<-performance(pred,measure="sens",x.measure="spec")
# plot(perf)
```

Figure 2.  Plot of the mean-squared-prediciton error for each level of the penalty parameter. Standard errors are derived with 30 fold cross-validation (which might be excessive for such a small data-set and lead to overconfidencein the precision of the estimate).  


#Grouped LASSO Regression

I also fit a grouped LASSO model for comparison to the regular LASSO. For this model I grouped all levels of a given factor together, but this step is flexible so if there is a more informative way to group variables we can incorporate that in the future.  As you can see from figures 3 and 4 the grouped LASSO removed variables at a faster rate than regular LASSO.

```{r GrpLASSO,echo=FALSE,message=FALSE}
#fit Group LASSO
library(grplasso)
#create groupings of variables
groups<-c(1,rep(2,length(levels(dat$rvp))-1), 3,4, rep(5,length(levels(dat$prev.stay))-1), 6,7,8, rep(9,length(levels(dat$curbf))-1), 10,11,12,13,14, rep(15,length(levels(dat$admit))-1),16:22)

# m2<-grplasso(x=X,y=as.matrix(Y),lambda=rev(seq(.5,2.5,.01)),center=T,standardize=T,index=groups)
# plot(m2)

library(gglasso)
#have to scale the vars first for this algorithm
x2<-apply(X[,2:37],2,scale)
x2<-cbind(X[,1],x2)
colnames(x2)[1]<-"(Intercept)"
y2<-ifelse(Y==1,1,-1)
m3<-gglasso(x=X,y=as.matrix(y2),loss='sqsvm',group=groups,lambda=seq(.05,3,.05))
plot(m3)
```

Figure 4. Regularization path for the grouped lasso model.  The penalty removes variables faster because of the grouping among factors.



```{r CVGLasso,echo=FALSE}
set.seed(37)
m3cv<-cv.gglasso(x=x2,y=as.matrix(y2),group=groups,pred.loss='loss',lambda=seq(.01,3,.01),nfolds=10,loss='sqsvm')
plot(m3cv)
gLowestMSE<-m3cv$gglasso.fit$beta[,which(m3cv$lambda==m3cv$lambda.min)]
combMat<-matrix(c(LowestMSE,gLowestMSE),37,2)
colnames(combMat)<-c("LASSO Coef","Grouped Coef")
rownames(combMat)<-rep("A",40)#dimnames(m3cv$gglasso.fit$beta)[[1]]
write.csv(combMat,"C:/Projects/Pneumonia/Pilot_coefs.csv")
```

Figure 5. Cross-validated 0-1 loss (a form of prediction error) for each value of the penalty coefficient.


#Important Variables

```{r CoefTable,echo=FALSE,results='asis',eval=TRUE}
library(pander)
library(xtable)
panderOptions("table.split.cells", Inf)
pandoc.table(combMat,style="rmarkdown", caption="Comparison of coefficients for the two LASSO methods.  Variables with non-zero coefficients are determined to be important.  It appears that grouped LASSO is more stringent than regular LASSO")

m4<-glm(Mortality~rvp+other.infect.stat+sex+prev.stay+immun.comp.stat+lung.dis.stat+diab+curbf+vpdose+aki+pl.eff+dysp+wheeze+admit+pna+mech.vent+ecmo+age+BMI+psi+hct,data=dat,family="binomial")
```


